{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b039b2ce",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete setup for Phi3:mini + OCR hybrid multimodal RAG\n",
    "Memory efficient solution for 16GB systems\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1d3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:13:31.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:13:31.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.730 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-19 13:13:31.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.747 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:31.749 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:32.261 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:32.265 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:13:32.265 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.651 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.659 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.676 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-19 13:15:51.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "chat_app.py\n",
    "100% local: Ollama (requests) + LlamaIndex (retriever) + HuggingFace embeddings + Streamlit UI\n",
    "No 'openai' package required. No API keys.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "\n",
    "# LlamaIndex imports (retriever + embeddings only)\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# ----------------------------\n",
    "# Safety: remove any OPENAI env var so nothing falls back accidentally\n",
    "# ----------------------------\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"  # Ollama server\n",
    "OLLAMA_CHAT_ENDPOINT = f\"{OLLAMA_BASE_URL}/v1/chat/completions\"\n",
    "DEFAULT_MODEL = \"phi3:mini\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Ollama client (requests)\n",
    "# ----------------------------\n",
    "def ollama_chat(messages, model=DEFAULT_MODEL, stream: bool = False, timeout: int = 300):\n",
    "    \"\"\"\n",
    "    messages: list of {\"role\": \"user\"/\"system\"/\"assistant\", \"content\": \"...\"}\n",
    "    If stream=True, yields text chunks (as they arrive).\n",
    "    If stream=False, returns the full string.\n",
    "    \"\"\"\n",
    "    payload = {\"model\": model, \"messages\": messages, \"stream\": stream}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    resp = requests.post(OLLAMA_CHAT_ENDPOINT, headers=headers, json=payload, stream=stream, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    if stream:\n",
    "        for raw_line in resp.iter_lines(decode_unicode=True):\n",
    "            if not raw_line:\n",
    "                continue\n",
    "            line = raw_line.strip()\n",
    "            if line.startswith(\"data:\"):\n",
    "                line = line[len(\"data:\"):].strip()\n",
    "            if not line or line == \"[DONE]\":\n",
    "                continue\n",
    "            try:\n",
    "                chunk = json.loads(line)\n",
    "                delta = chunk.get(\"choices\", [])[0].get(\"delta\", {})\n",
    "                content_piece = delta.get(\"content\")\n",
    "                if content_piece:\n",
    "                    yield content_piece\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        return\n",
    "    else:\n",
    "        data = resp.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Build / load index\n",
    "# ----------------------------\n",
    "def build_index(data_dir: Path = DATA_DIR, embed_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Builds a VectorStoreIndex from files in data_dir using HuggingFace embeddings only.\n",
    "    \"\"\"\n",
    "    Settings.embed_model = HuggingFaceEmbedding(model_name=embed_model_name, device=\"cpu\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(str(data_dir))\n",
    "    docs = reader.load_data()\n",
    "    if not docs:\n",
    "        return None\n",
    "    return VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "# ----------------------------\n",
    "# Node -> text extractor\n",
    "# ----------------------------\n",
    "def node_to_text(node) -> str:\n",
    "    \"\"\"Try to extract readable text from LlamaIndex node types.\"\"\"\n",
    "    try:\n",
    "        return node.get_content()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        inner = getattr(node, \"node\", None)\n",
    "        if inner and hasattr(inner, \"get_content\"):\n",
    "            return inner.get_content()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return getattr(node, \"text\", None) or getattr(node, \"get_text\", lambda: None)() or str(node)\n",
    "    except Exception:\n",
    "        return str(node)\n",
    "\n",
    "# ----------------------------\n",
    "# Streamlit UI\n",
    "# ----------------------------\n",
    "st.set_page_config(page_title=\"Local PDF Q&A (Ollama only)\", layout=\"wide\")\n",
    "st.title(\"ðŸ“š Local PDF Q&A â€” Ollama only (no OpenAI)\")\n",
    "\n",
    "# Sidebar: upload & index\n",
    "with st.sidebar:\n",
    "    st.header(\"Index / Data\")\n",
    "    uploaded = st.file_uploader(\"Upload PDFs / text files\", accept_multiple_files=True)\n",
    "\n",
    "    if uploaded:\n",
    "        for f in uploaded:\n",
    "            dest = DATA_DIR / f.name\n",
    "            with open(dest, \"wb\") as out:\n",
    "                out.write(f.getbuffer())\n",
    "        st.success(f\"Saved {len(uploaded)} files. Rebuild index below.\")\n",
    "\n",
    "        if \"index\" in st.session_state:\n",
    "            st.session_state.pop(\"index\")\n",
    "\n",
    "    if st.button(\"Rebuild index now\"):\n",
    "        if \"index\" in st.session_state:\n",
    "            st.session_state.pop(\"index\")\n",
    "        st.experimental_rerun()\n",
    "\n",
    "    st.write(\"---\")\n",
    "    model_choice = st.text_input(\"Ollama model name\", value=DEFAULT_MODEL)\n",
    "    if model_choice:\n",
    "        DEFAULT_MODEL = model_choice\n",
    "\n",
    "# Build index if needed\n",
    "if \"index\" not in st.session_state:\n",
    "    with st.spinner(\"Building index from ./data...\"):\n",
    "        idx = build_index(DATA_DIR)\n",
    "        st.session_state[\"index\"] = idx\n",
    "\n",
    "index = st.session_state.get(\"index\")\n",
    "retriever = index.as_retriever(similarity_top_k=3) if index else None\n",
    "\n",
    "# Chat state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "for m in st.session_state[\"messages\"]:\n",
    "    with st.chat_message(m[\"role\"]):\n",
    "        st.markdown(m[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "prompt = st.chat_input(\"Ask something about your PDFs...\")\n",
    "\n",
    "if prompt:\n",
    "    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    retrieved_text = \"\"\n",
    "    if retriever:\n",
    "        try:\n",
    "            nodes = retriever.retrieve(prompt)\n",
    "            pieces = [node_to_text(n).strip() for n in nodes if node_to_text(n).strip()]\n",
    "            retrieved_text = \"\\n\\n\".join(pieces[:6])\n",
    "        except Exception as e:\n",
    "            st.error(f\"Retrieval error: {e}\")\n",
    "\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. Use the context to answer. If missing, say you don't know.\"\n",
    "    }\n",
    "    user_with_context = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Context:\\n\\n{retrieved_text}\\n\\nQuestion: {prompt}\"\n",
    "    }\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        placeholder = st.empty()\n",
    "        assistant_text = \"\"\n",
    "        try:\n",
    "            for chunk in ollama_chat([system_msg, user_with_context], model=DEFAULT_MODEL, stream=True):\n",
    "                assistant_text += chunk\n",
    "                placeholder.markdown(assistant_text + \"â–Œ\")\n",
    "            placeholder.markdown(assistant_text)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Ollama error: {e}\")\n",
    "            assistant_text = f\"(error) {e}\"\n",
    "\n",
    "    st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": assistant_text})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
